{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkLTT3nMtitDwFqQ4klIba",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanina-Kutovaya/GNN/blob/main/notebooks/catboost_stratified_split.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Функция stratified_train_test_validation_split для загрузки данных из PostgreSQL в Сatboost для данных, которые не помещаются в оперативную память, но помещаются на диск."
      ],
      "metadata": {
        "id": "b6oQmeVe8-TW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В классе с unit-тестами ```TestStratifiedSplit``` реализованные тесты:\n",
        "\n",
        "- ```test_invalid_train_val_size``` - Обработка ошибочных значений ```train_size``` и ```val_size```\n",
        "- ```test_sql_queries_built_correctly``` - Корректное формирование SQL-запросов\n",
        "- ```test_pool_objects_returned``` - Возвращаются ли объекты ```Сatboost Pool```\n",
        "- ```test_class_weights_calculated_correctly``` - Верный расчет весов классов\n",
        "- ```test_test_data_saved_to_file``` - Сохранение тестовой выборки в CSV"
      ],
      "metadata": {
        "id": "4YRHsAwo-HV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q catboost"
      ],
      "metadata": {
        "id": "MqpGkDgIhNEo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import psycopg2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from catboost import Pool\n",
        "\n",
        "# Настройка логгера\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "def stratified_train_test_validation_split(\n",
        "    connection_params,\n",
        "    table_name,\n",
        "    target_column,\n",
        "    id_column='id',\n",
        "    train_size=0.6,\n",
        "    val_size=0.2,\n",
        "    batch_size=1000,\n",
        "    seed=42,\n",
        "    output_test_file=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Разделяет данные из PostgreSQL на train/validation/test с сохранением пропорций классов.\n",
        "    Тестовые данные сохраняются в CSV-файл для инференса.\n",
        "    Возвращает: train_pool, val_pool, class_weights\n",
        "    \"\"\"\n",
        "    logger.info(\"Начало разбиения данных\")\n",
        "\n",
        "    test_size = 1.0 - train_size - val_size\n",
        "    if test_size < 0:\n",
        "        logger.error(\"Сумма train_size и val_size больше 1\")\n",
        "        raise ValueError(\"Сумма train_size и val_size должна быть <= 1.\")\n",
        "\n",
        "    conn = None\n",
        "    cursor = None\n",
        "\n",
        "    try:\n",
        "        # Подключение к БД\n",
        "        conn = psycopg2.connect(**connection_params)\n",
        "        cursor = conn.cursor()\n",
        "        logger.info(\"Подключено к базе данных\")\n",
        "\n",
        "        # Получение списка столбцов\n",
        "        cursor.execute(f\"SELECT column_name FROM information_schema.columns WHERE table_name='{table_name}'\")\n",
        "        columns = [row[0] for row in cursor.fetchall()]\n",
        "        feature_columns = [col for col in columns if col != target_column]\n",
        "        logger.info(f\"Определены {len(feature_columns)} фичей: {feature_columns}\")\n",
        "\n",
        "        # Статистика классов\n",
        "        cursor.execute(f\"SELECT {target_column}, COUNT(*) FROM {table_name} GROUP BY {target_column}\")\n",
        "        class_counts = dict(cursor.fetchall())\n",
        "        logger.info(f\"Количество записей по классам: {class_counts}\")\n",
        "\n",
        "        # Веса классов\n",
        "        total_samples = sum(class_counts.values())\n",
        "        class_weights = {\n",
        "            cls: total_samples / (len(class_counts) * count)\n",
        "            for cls, count in class_counts.items()\n",
        "        }\n",
        "        logger.info(f\"Вычислены веса классов: {class_weights}\")\n",
        "\n",
        "        # Построение SQL-запросов\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        def build_query(split):\n",
        "            queries = []\n",
        "            for cls, count in class_counts.items():\n",
        "                total_cls = count\n",
        "                if split == 'train':\n",
        "                    start = 0\n",
        "                    end = int(total_cls * train_size)\n",
        "                elif split == 'val':\n",
        "                    start = int(total_cls * train_size)\n",
        "                    end = start + int(total_cls * val_size)\n",
        "                else:\n",
        "                    start = int(total_cls * (train_size + val_size))\n",
        "                    end = total_cls\n",
        "                query = f\"\"\"\n",
        "                    SELECT * FROM (\n",
        "                        SELECT *,\n",
        "                        ABS(HASHTEXT({id_column}::TEXT || '{seed}')) % 10000 AS hash_val\n",
        "                        FROM {table_name}\n",
        "                        WHERE {target_column} = '{cls}'\n",
        "                    ) AS sub\n",
        "                    WHERE hash_val BETWEEN {start} AND {end - 1}\n",
        "                \"\"\"\n",
        "                queries.append(query)\n",
        "            return \" UNION ALL \".join(queries)\n",
        "\n",
        "        train_query = build_query('train')\n",
        "        val_query = build_query('val')\n",
        "        test_query = build_query('test')\n",
        "\n",
        "        # Класс итератора\n",
        "        class PostgreSQLPoolIterator:\n",
        "            def __init__(self, connection_params, query, feature_cols, target_col, class_weights, batch_size):\n",
        "                self.connection_params = connection_params\n",
        "                self.query = query\n",
        "                self.feature_cols = feature_cols\n",
        "                self.target_col = target_col\n",
        "                self.class_weights = class_weights\n",
        "                self.batch_size = batch_size\n",
        "                self.conn = None\n",
        "                self.cursor = None\n",
        "                self.colnames = None\n",
        "\n",
        "            def __iter__(self):\n",
        "                self.conn = psycopg2.connect(**self.connection_params)\n",
        "                self.cursor = self.conn.cursor()\n",
        "                self.cursor.execute(self.query)\n",
        "                self.colnames = [desc[0] for desc in self.cursor.description]\n",
        "                return self\n",
        "\n",
        "            def __next__(self):\n",
        "                records = self.cursor.fetchmany(self.batch_size)\n",
        "                if not records:\n",
        "                    self.cursor.close()\n",
        "                    self.conn.close()\n",
        "                    raise StopIteration\n",
        "                df = pd.DataFrame(records, columns=self.colnames)\n",
        "                features = df[self.feature_cols].values\n",
        "                target = df[self.target_col].values\n",
        "                weights = df[self.target_col].map(self.class_weights).values\n",
        "                return features, target, weights\n",
        "\n",
        "            def get_all_data(self):\n",
        "                \"\"\"Загружает все данные в numpy-массивы\"\"\"\n",
        "                all_features = []\n",
        "                all_target = []\n",
        "                all_weights = []\n",
        "                for features, target, weights in self:\n",
        "                    all_features.append(features)\n",
        "                    all_target.append(target)\n",
        "                    all_weights.append(weights)\n",
        "\n",
        "                if len(all_features) == 0:\n",
        "                    logger.warning(\"Нет данных для обучения\")\n",
        "                    feature_shape = (1, len(self.feature_cols))\n",
        "                    return (\n",
        "                        np.zeros(feature_shape, dtype=np.float32),\n",
        "                        np.array([0], dtype=np.int64),\n",
        "                        np.array([1.0], dtype=np.float32)\n",
        "                    )\n",
        "\n",
        "                return (\n",
        "                    np.vstack(all_features),\n",
        "                    np.concatenate(all_target),\n",
        "                    np.concatenate(all_weights)\n",
        "                )\n",
        "\n",
        "        # Создание итераторов\n",
        "        train_iter = PostgreSQLPoolIterator(\n",
        "            connection_params, train_query, feature_columns, target_column, class_weights, batch_size\n",
        "        )\n",
        "        val_iter = PostgreSQLPoolIterator(\n",
        "            connection_params, val_query, feature_columns, target_column, class_weights, batch_size\n",
        "        )\n",
        "\n",
        "        train_data, train_label, train_weight = train_iter.get_all_data()\n",
        "        val_data, val_label, val_weight = val_iter.get_all_data()\n",
        "\n",
        "        train_pool = Pool(\n",
        "            data=train_data,\n",
        "            label=train_label,\n",
        "            weight=train_weight,\n",
        "            feature_names=feature_columns\n",
        "        )\n",
        "        val_pool = Pool(\n",
        "            data=val_data,\n",
        "            label=val_label,\n",
        "            weight=val_weight,\n",
        "            feature_names=feature_columns\n",
        "        )\n",
        "\n",
        "        # Сохранение тестовых данных в CSV\n",
        "        if output_test_file:\n",
        "            logger.info(f\"Сохранение тестовых данных в файл: {output_test_file}\")\n",
        "            test_iter = PostgreSQLPoolIterator(\n",
        "                connection_params, test_query, feature_columns, target_column, class_weights, batch_size\n",
        "            )\n",
        "            with open(output_test_file, 'w') as f:\n",
        "                header = ','.join(feature_columns + [target_column, 'weight'])\n",
        "                f.write(header + '\\n')\n",
        "                for batch in test_iter:\n",
        "                    features, target, weights = batch\n",
        "                    df = pd.DataFrame(features, columns=feature_columns)\n",
        "                    df[target_column] = target\n",
        "                    df['weight'] = weights\n",
        "                    df.to_csv(f, index=False, header=False, mode='a')\n",
        "\n",
        "        logger.info(\"Разделение данных успешно завершено\")\n",
        "        return train_pool, val_pool, class_weights\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(\"Ошибка при разделении данных\", exc_info=True)\n",
        "        raise\n",
        "    finally:\n",
        "        if cursor:\n",
        "            cursor.close()\n",
        "        if conn:\n",
        "            conn.close()\n",
        "        logger.info(\"Соединение с БД закрыто\")\n",
        "\n",
        "\n",
        "import unittest\n",
        "from unittest.mock import patch, MagicMock, Mock\n",
        "import os\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import Pool\n",
        "import logging\n",
        "\n",
        "\n",
        "class TestStratifiedSplit(unittest.TestCase):\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        \"\"\"Настройка логгирования перед запуском всех тестов\"\"\"\n",
        "        cls.original_log_level = logging.getLogger().level\n",
        "        logging.disable(logging.CRITICAL)  # Отключаем логи для чистоты вывода\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"Восстанавливаем уровень логгирования после всех тестов\"\"\"\n",
        "        logging.disable(cls.original_log_level)\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"Настройка мока для всех тестов\"\"\"\n",
        "        self.mock_connect_patcher = patch('psycopg2.connect')\n",
        "        self.mock_connect = self.mock_connect_patcher.start()\n",
        "        self.mock_cursor = Mock()\n",
        "        self.mock_connect.return_value.cursor.return_value = self.mock_cursor\n",
        "\n",
        "        self.columns = [('id',), ('feature1',), ('feature2',), ('target',)]\n",
        "        self.class_counts = [('class1', 60), ('class2', 40)]\n",
        "        self.feature_columns = ['feature1', 'feature2']\n",
        "        self.executed_queries = []\n",
        "\n",
        "        def execute_side_effect(query, *args, **kwargs):\n",
        "            self.executed_queries.append(query)\n",
        "            return None\n",
        "\n",
        "        self.mock_cursor.execute.side_effect = execute_side_effect\n",
        "\n",
        "        self.data_chunks = [\n",
        "            [(1, 1.1, 2.1, 'class1')] * 10,\n",
        "            [(1, 1.1, 2.1, 'class1')] * 10,\n",
        "            [(2, 1.2, 2.2, 'class2')] * 10,\n",
        "            [(2, 1.2, 2.2, 'class2')] * 10,\n",
        "        ]\n",
        "        self._call_count = 0\n",
        "\n",
        "        def fetchmany_side_effect(batch_size):\n",
        "            if self._call_count >= len(self.data_chunks):\n",
        "                return []\n",
        "            result = self.data_chunks[self._call_count]\n",
        "            self._call_count += 1\n",
        "            return result\n",
        "\n",
        "        self.mock_cursor.fetchmany.side_effect = fetchmany_side_effect\n",
        "\n",
        "        self.mock_cursor.fetchall.side_effect = [\n",
        "            self.columns,\n",
        "            self.class_counts,\n",
        "        ]\n",
        "\n",
        "        self.mock_cursor.description = [\n",
        "            ('id', None), ('feature1', None), ('feature2', None), ('target', None)\n",
        "        ]\n",
        "\n",
        "        self.connection_params = {\n",
        "            'dbname': 'test_db',\n",
        "            'user': 'user',\n",
        "            'password': 'pass',\n",
        "            'host': 'localhost'\n",
        "        }\n",
        "\n",
        "    def tearDown(self):\n",
        "        \"\"\"Очистка после тестов\"\"\"\n",
        "        self.mock_connect_patcher.stop()\n",
        "\n",
        "    def test_invalid_train_val_size_raises_value_error(self):\n",
        "        \"\"\"\n",
        "        Проверяет, что функция вызывает ValueError при некорректных размерах выборки.\n",
        "        \"\"\"\n",
        "        with self.assertRaises(ValueError) as context:\n",
        "            stratified_train_test_validation_split(\n",
        "                self.connection_params,\n",
        "                'test_table',\n",
        "                'target',\n",
        "                train_size=0.7,\n",
        "                val_size=0.4\n",
        "            )\n",
        "        expected_msg = \"Сумма train_size и val_size должна быть <= 1.\"\n",
        "        self.assertEqual(str(context.exception), expected_msg)\n",
        "\n",
        "    def test_sql_queries_built_correctly(self):\n",
        "        \"\"\"\n",
        "        Проверяет, что SQL-запросы формируются корректно.\n",
        "        \"\"\"\n",
        "        _, _, _ = stratified_train_test_validation_split(\n",
        "            self.connection_params,\n",
        "            'test_table',\n",
        "            'target',\n",
        "            id_column='id',\n",
        "            train_size=0.6,\n",
        "            val_size=0.2,\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        column_query = (\n",
        "            \"SELECT column_name FROM information_schema.columns WHERE table_name='test_table'\"\n",
        "        )\n",
        "        count_query = (\n",
        "            \"SELECT target, COUNT(*) FROM test_table GROUP BY target\"\n",
        "        )\n",
        "\n",
        "        self.assertIn(column_query, self.executed_queries)\n",
        "        self.assertIn(count_query, self.executed_queries)\n",
        "\n",
        "        for query in self.executed_queries:\n",
        "            if 'BETWEEN' in query:\n",
        "                self.assertIn(\"ABS(HASHTEXT\", query)\n",
        "                self.assertIn(\"BETWEEN\", query)\n",
        "\n",
        "    def test_pool_objects_returned_with_valid_data(self):\n",
        "        \"\"\"\n",
        "        Проверяет, что возвращаются объекты Pool при корректных данных.\n",
        "        \"\"\"\n",
        "        train_pool, val_pool, class_weights = stratified_train_test_validation_split(\n",
        "            self.connection_params,\n",
        "            'test_table',\n",
        "            'target',\n",
        "            id_column='id',\n",
        "            train_size=0.6,\n",
        "            val_size=0.2,\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        self.assertIsInstance(train_pool, Pool)\n",
        "        self.assertIsInstance(val_pool, Pool)\n",
        "        self.assertGreater(len(class_weights), 0)\n",
        "\n",
        "    def test_class_weights_calculated_correctly(self):\n",
        "        \"\"\"\n",
        "        Проверяет вычисление весов классов.\n",
        "        \"\"\"\n",
        "        _, _, class_weights = stratified_train_test_validation_split(\n",
        "            self.connection_params,\n",
        "            'test_table',\n",
        "            'target',\n",
        "            id_column='id',\n",
        "            train_size=0.6,\n",
        "            val_size=0.2,\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        total_samples = sum(count for _, count in self.class_counts)\n",
        "        expected_weights = {\n",
        "            cls: total_samples / (len(self.class_counts) * count)\n",
        "            for cls, count in self.class_counts\n",
        "        }\n",
        "\n",
        "        for cls in class_weights:\n",
        "            self.assertAlmostEqual(class_weights[cls], expected_weights[cls], delta=1e-6)\n",
        "\n",
        "    def test_test_data_saved_to_file_correctly(self):\n",
        "        \"\"\"\n",
        "        Проверяет сохранение тестовых данных в CSV файл.\n",
        "        \"\"\"\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmpfile:\n",
        "            filename = tmpfile.name\n",
        "\n",
        "        try:\n",
        "            _, _, _ = stratified_train_test_validation_split(\n",
        "                self.connection_params,\n",
        "                'test_table',\n",
        "                'target',\n",
        "                id_column='id',\n",
        "                train_size=0.6,\n",
        "                val_size=0.2,\n",
        "                seed=42,\n",
        "                output_test_file=filename\n",
        "            )\n",
        "\n",
        "            df = pd.read_csv(filename)\n",
        "            self.assertTrue(len(df.columns) > 0)\n",
        "            self.assertIn('target', df.columns)\n",
        "            self.assertIn('weight', df.columns)\n",
        "            self.assertTrue(all(col in df.columns for col in self.feature_columns))\n",
        "        finally:\n",
        "            if os.path.exists(filename):\n",
        "                os.remove(filename)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=[''], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMOt5YVFj24X",
        "outputId": "0790b0b9-3467-4ba6-ee0b-c5a09db8513f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".....\n",
            "----------------------------------------------------------------------\n",
            "Ran 5 tests in 0.081s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    }
  ]
}